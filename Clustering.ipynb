{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MqRI8zd0N-o",
        "outputId": "eee63cd2-e429-49f8-8de6-3f349b10dd9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.10/dist-packages (0.8.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspellchecker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yasQ-nxV1ikT",
        "outputId": "39f5aa2f-14f2-4acb-9122-3049e82ceeb1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from spellchecker import SpellChecker\n",
        "from sklearn.cluster import DBSCAN\n",
        "from collections import Counter\n",
        "import time\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "# Download the spellchecker dictionary\n",
        "spell = SpellChecker()\n",
        "spell.distance = 1  # Set the maximum edit distance for a word to be considered a correction\n",
        "# spell.distance = 2  # Experimented with this value but decided not to proceed due to efficiency concerns\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def clean_text(text):\n",
        "    # Clean and preprocess the text\n",
        "\n",
        "    # Get the default stop words set\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    # Define a list of additional stop words\n",
        "    additional_stop_words = [\"amp\", \"rt\", \"via\"]\n",
        "    # Update the stop words set with the additional stop words list\n",
        "    stop_words.update(additional_stop_words)\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    corrected_tokens = [spell.correction(word) for word in tokens]\n",
        "    corrected_tokens = [word for word in tokens if word.isalpha()]\n",
        "    corrected_tokens = [word for word in tokens if word not in stop_words]\n",
        "    return ' '.join(corrected_tokens)\n",
        "\n",
        "def vectorize_text(df):\n",
        "    # Vectorize the cleaned text using TF-IDF\n",
        "    vectorizer = TfidfVectorizer(max_features=1000)\n",
        "    X = vectorizer.fit_transform(df['Cleaned_Text'])\n",
        "    return X, vectorizer\n",
        "\n",
        "def perform_kmeans(X, num_clusters=5):\n",
        "    # Perform KMeans clustering\n",
        "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "    cluster_labels = kmeans.fit_predict(X)\n",
        "    return cluster_labels, kmeans\n",
        "\n",
        "# Experimented with DBScan Clustering but decided not to proceed due to efficiency concerns\n",
        "def perform_dbscan(X, eps=0.5, min_samples=5):\n",
        "    # Reduce dimensionality for efficiency\n",
        "    svd = TruncatedSVD(n_components=50, random_state=42)\n",
        "    X_svd = svd.fit_transform(X)\n",
        "\n",
        "    # Perform DBSCAN clustering\n",
        "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "    cluster_labels = dbscan.fit_predict(X_svd)\n",
        "    return cluster_labels, dbscan\n",
        "\n",
        "def visualize_clusters(X_svd, cluster_labels):\n",
        "    # Visualize clusters in 2D using Truncated SVD\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    scatter = plt.scatter(X_svd[:, 0], X_svd[:, 1], c=cluster_labels, cmap='viridis')\n",
        "    plt.title('Clusters Visualization (2D)')\n",
        "    plt.xlabel('SVD Component 1')\n",
        "    plt.ylabel('SVD Component 2')\n",
        "    plt.colorbar(scatter, label='Cluster Labels')\n",
        "    plt.show()\n",
        "\n",
        "def generate_wordclouds(df, vectorizer, num_clusters=5, terms_per_cluster=5):\n",
        "    # Generate word clouds for each cluster\n",
        "    for cluster_label in range(num_clusters):\n",
        "        cluster_data = df[df['Cluster_Labels'] == cluster_label]\n",
        "        words_in_cluster = ' '.join(cluster_data['Cleaned_Text'])\n",
        "\n",
        "        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(words_in_cluster)\n",
        "\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.imshow(wordcloud, interpolation='bilinear')\n",
        "        plt.title(f'Word Cloud for Cluster {cluster_label}')\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "def print_common_terms(terms, order_centroids, num_clusters=5, terms_per_cluster=25):\n",
        "    # Describe important terms in each cluster\n",
        "    for i in range(num_clusters):\n",
        "        print(f'\\nCluster {i} - Important Terms:')\n",
        "        for j in range(terms_per_cluster):\n",
        "            print(f'- {terms[order_centroids[i, j]]}')\n",
        "\n",
        "def print_common_terms_dbscan(df, terms, cluster_labels, num_clusters=5, top_terms=25):\n",
        "    # Print common terms within each cluster\n",
        "    for cluster_label in range(num_clusters):\n",
        "        print(f'\\nCluster {cluster_label} - Common Terms:')\n",
        "        cluster_indices = df.index[df['Cluster_Labels'] == cluster_label].tolist()\n",
        "\n",
        "        # Combine all cleaned text within the cluster\n",
        "        cluster_text = ' '.join(df['Cleaned_Text'][cluster_indices])\n",
        "\n",
        "        # Tokenize and count term frequencies\n",
        "        cluster_tokens = word_tokenize(cluster_text)\n",
        "        term_freq = Counter(cluster_tokens)\n",
        "\n",
        "        # Print the most common terms\n",
        "        for term, freq in term_freq.most_common(top_terms):\n",
        "            print(f'- {term}: {freq} occurrences')\n",
        "\n",
        "# Function to describe the main topic of each cluster\n",
        "def describe_topic(cluster_label):\n",
        "    return cluster_labels_mapping.get(cluster_label, 'Unknown Topic')\n",
        "\n",
        "# Function to suggest user accounts based on the identified topics\n",
        "def suggest_cluster_name(important_terms):\n",
        "    # Your logic to suggest a name based on important terms\n",
        "    # Example: If 'apple' or 'aapl' is present, suggest 'Financial Markets and Stocks'\n",
        "    if any(term in important_terms for term in ['apple', 'aapl']):\n",
        "        return 'Financial Markets and Stocks'\n",
        "    # Example: If 'covid' or 'coronavirus' is present, suggest 'Public Health and Pandemic Updates'\n",
        "    elif any(term in important_terms for term in ['covid', 'coronavirus']):\n",
        "        return 'Public Health and Pandemic Updates'\n",
        "    # Example: If 'politics' or 'trump' is present, suggest 'Political Affairs and News'\n",
        "    elif any(term in important_terms for term in ['politics', 'trump']):\n",
        "        return 'Political Affairs and News'\n",
        "    # Example: If 'worldcup' or 'fifa' is present, suggest 'Global Football Events'\n",
        "    elif any(term in important_terms for term in ['worldcup', 'fifa']):\n",
        "        return 'Global Football Events'\n",
        "    # Example: If 'tech' or 'google' is present, suggest 'Technology Trends and Innovations'\n",
        "    elif any(term in important_terms for term in ['tech', 'google']):\n",
        "        return 'Technology Trends and Innovations'\n",
        "    # Add more conditions as needed\n",
        "    else:\n",
        "        return 'Unknown Cluster'\n",
        "\n",
        "\n",
        "# Function to apply topic description and user account suggestions to the DataFrame\n",
        "def apply_topic_suggestions(df):\n",
        "    df['Topic_Description'] = df['Cluster_Labels'].apply(describe_topic)\n",
        "    df['Suggested_Accounts'] = df['Topic_Description'].apply(suggest_accounts)\n",
        "    return df[['Cleaned_Text', 'Cluster_Labels', 'Topic_Description', 'Suggested_Accounts']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpQk9IWxkeUw"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics import silhouette_score\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def train_subtopic_models(df, major_topic_column='Topic_Description', num_clusters=5):\n",
        "    # Create a dictionary to store subtopic models and related information\n",
        "    subtopic_models = {}\n",
        "\n",
        "    # Iterate over major topics\n",
        "    for major_topic in df[major_topic_column].unique():\n",
        "        # Create a subset DataFrame for the current major topic\n",
        "        major_topic_subset = df[df[major_topic_column] == major_topic]\n",
        "\n",
        "        # Vectorize text for the current major topic subset\n",
        "        X_major_topic, vectorizer_major_topic = vectorize_text(major_topic_subset)\n",
        "\n",
        "        # Perform KMeans clustering on the major topic subset\n",
        "        cluster_labels_major_topic, kmeans_major_topic = perform_kmeans(X_major_topic, num_clusters)\n",
        "\n",
        "        # Add cluster labels to the major topic subset DataFrame\n",
        "        major_topic_subset['Subtopic_Labels'] = cluster_labels_major_topic\n",
        "\n",
        "        # Store subtopic models and related information in the dictionary\n",
        "        subtopic_models[major_topic] = {\n",
        "            'vectorizer': vectorizer_major_topic,\n",
        "            'kmeans': kmeans_major_topic,\n",
        "            'subset_df': major_topic_subset\n",
        "        }\n",
        "\n",
        "    return subtopic_models\n",
        "\n",
        "def generate_subtopic_labels(subtopic_models):\n",
        "    # Generate subtopic labels based on prominent terms in each subtopic cluster\n",
        "    subtopic_labels = {}\n",
        "\n",
        "    for major_topic, model_info in subtopic_models.items():\n",
        "        # Get important terms for each subtopic cluster\n",
        "        terms_subtopic = model_info['vectorizer'].get_feature_names_out()\n",
        "        order_centroids_subtopic = model_info['kmeans'].cluster_centers_.argsort()[:, ::-1]\n",
        "\n",
        "        # Get the top term for each subtopic cluster\n",
        "        top_terms_subtopic = [terms_subtopic[order_centroids_subtopic[i, 0]] for i in range(len(order_centroids_subtopic))]\n",
        "\n",
        "        # Store subtopic labels in the dictionary\n",
        "        subtopic_labels[major_topic] = top_terms_subtopic\n",
        "\n",
        "    return subtopic_labels\n",
        "\n",
        "def analyze_subtopics(subtopic_models, subtopic_labels):\n",
        "    # Analyze identified subtopics\n",
        "    for major_topic, model_info in subtopic_models.items():\n",
        "        print(f\"\\nMajor Topic: {major_topic}\")\n",
        "        print(\"Top Terms in Subtopics:\")\n",
        "        for i, label in enumerate(subtopic_labels[major_topic]):\n",
        "            print(f\"Subtopic {i}: {label}\")\n",
        "            # Add further analysis if needed\n",
        "\n",
        "\n",
        "def evaluate_subtopic_models(subtopic_models):\n",
        "    # Evaluate subtopic models using silhouette score\n",
        "    for major_topic, model_info in subtopic_models.items():\n",
        "        X_subtopic = model_info['subset_df'].drop(columns=['Subtopic_Labels'])\n",
        "\n",
        "        # Calculate silhouette score for subtopic clusters\n",
        "        silhouette_avg = silhouette_score(X_subtopic, model_info['subset_df']['Subtopic_Labels'])\n",
        "        print(f\"\\nEvaluation for Major Topic '{major_topic}':\")\n",
        "        print(f\"Silhouette Score for Subtopic Clusters: {silhouette_avg}\")\n",
        "\n",
        "def visualize_subtopic_clusters(X_svd, subtopic_labels, major_topic):\n",
        "    # Visualize subtopic clusters in 2D using Truncated SVD\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    scatter = plt.scatter(X_svd[:, 0], X_svd[:, 1], c=subtopic_labels[major_topic], cmap='viridis')\n",
        "    plt.title(f'Subtopic Clusters Visualization for Major Topic \"{major_topic}\" (2D)')\n",
        "    plt.xlabel('SVD Component 1')\n",
        "    plt.ylabel('SVD Component 2')\n",
        "    plt.colorbar(scatter, label='Subtopic Labels')\n",
        "    plt.show()\n",
        "\n",
        "def generate_subtopic_wordclouds(df, vectorizer, subtopic_models, major_topic, terms_per_subtopic=10):\n",
        "    # Generate word clouds for each subtopic within a major topic\n",
        "    model_info = subtopic_models[major_topic]\n",
        "    terms_subtopic = model_info['vectorizer'].get_feature_names_out()\n",
        "    order_centroids_subtopic = model_info['kmeans'].cluster_centers_.argsort()[:, ::-1]\n",
        "\n",
        "    for subtopic_label in range(len(order_centroids_subtopic)):\n",
        "        terms_in_subtopic = [terms_subtopic[order_centroids_subtopic[subtopic_label, j]] for j in range(terms_per_subtopic)]\n",
        "        words_in_subtopic = ' '.join(terms_in_subtopic)\n",
        "\n",
        "        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(words_in_subtopic)\n",
        "\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.imshow(wordcloud, interpolation='bilinear')\n",
        "        plt.title(f'Word Cloud for Subtopic {subtopic_label} in Major Topic \"{major_topic}\"')\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZz0eaJO1m9B",
        "outputId": "5fbb6e02-e718-4cc5-a2e3-5ae757fd4592"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time taken to read data: 3.31 seconds\n",
            "Time taken to clean and preprocess text: 949.37 seconds\n",
            "DataFrame saved to /content/drive/My Drive/4404MLProject/Cleaned_Data.csv\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# Specify the path to your Excel file\n",
        "file_path = \"/content/drive/My Drive/4404MLProject/Data.csv\"\n",
        "\n",
        "# Read INPUT FILE\n",
        "start_time = time.time()\n",
        "df = pd.read_csv(file_path,header=None , names=['Tweet_Text'])\n",
        "print(f\"Time taken to read data: {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "# Clean and preprocess text\n",
        "start_time = time.time()\n",
        "df['Cleaned_Text'] = df['Tweet_Text'].apply(clean_text)\n",
        "df['Cleaned_Text'] = df['Cleaned_Text'].fillna('')\n",
        "print(f\"Time taken to clean and preprocess text: {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "# Save the DataFrame to an Excel file\n",
        "output_file_path = \"/content/drive/My Drive/4404MLProject/Cleaned_Data.csv\"\n",
        "df.to_csv(output_file_path, index=False)\n",
        "print(f\"DataFrame saved to {output_file_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LbAq-X-FyPsA",
        "outputId": "05e365cd-c07d-4914-853e-2bc3db40ba10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time taken to read data: 1.49 seconds\n"
          ]
        }
      ],
      "source": [
        "# # IN ORDER TO SKIP THE PREPROCESSING AND IMPORTING CLEANED_DATA DIRECTLY, UNCOMMENT THE BELOW CODE\n",
        "# Specify the path to your Excel file\n",
        "file_path = \"/content/drive/My Drive/4404MLProject/Cleaned_Data.csv\"\n",
        "\n",
        "# Read data\n",
        "start_time = time.time()\n",
        "df = pd.read_csv(file_path,header=None, names=['Tweet_Text', 'Cleaned_Text'])\n",
        "df['Cleaned_Text'] = df['Cleaned_Text'].fillna('')\n",
        "\n",
        "print(f\"Time taken to read data: {time.time() - start_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yiqfd4ZX56Wt",
        "outputId": "b6125dce-8e50-4d01-987b-abbadabc54b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time taken to vectorize text: 4.30 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time taken for KMeans clustering: 5.98 seconds\n",
            "Time taken for dimensionality reduction: 1.45 seconds\n",
            "\n",
            "Cluster 0 - Important Terms:\n",
            "- politics\n",
            "- news\n",
            "- us\n",
            "- trump\n",
            "- tsla\n",
            "- amzn\n",
            "- new\n",
            "- people\n",
            "- goog\n",
            "- google\n",
            "- msft\n",
            "- political\n",
            "- get\n",
            "- like\n",
            "- one\n",
            "\n",
            "Cluster 1 - Important Terms:\n",
            "- cases\n",
            "- new\n",
            "- covid\n",
            "- deaths\n",
            "- total\n",
            "- coronavirus\n",
            "- reported\n",
            "- confirmed\n",
            "- india\n",
            "- active\n",
            "- last\n",
            "- positive\n",
            "- reports\n",
            "- hours\n",
            "- number\n",
            "\n",
            "Cluster 2 - Important Terms:\n",
            "- aapl\n",
            "- apple\n",
            "- fb\n",
            "- goog\n",
            "- googl\n",
            "- earnings\n",
            "- spy\n",
            "- iphone\n",
            "- stock\n",
            "- options\n",
            "- stocks\n",
            "- yhoo\n",
            "- lnkd\n",
            "- qqq\n",
            "- baba\n",
            "\n",
            "Cluster 3 - Important Terms:\n",
            "- worldcup\n",
            "- qatar\n",
            "- cup\n",
            "- fifaworldcup\n",
            "- world\n",
            "- ecuador\n",
            "- qatarworldcup\n",
            "- opening\n",
            "- fifa\n",
            "- goal\n",
            "- offside\n",
            "- football\n",
            "- ceremony\n",
            "- worldcupqatar\n",
            "- var\n",
            "\n",
            "Cluster 4 - Important Terms:\n",
            "- covid\n",
            "- coronavirus\n",
            "- pandemic\n",
            "- people\n",
            "- vaccine\n",
            "- us\n",
            "- new\n",
            "- positive\n",
            "- health\n",
            "- due\n",
            "- testing\n",
            "- deaths\n",
            "- spread\n",
            "- one\n",
            "- need\n",
            "Time taken to analyze important terms: 0.02 seconds\n"
          ]
        }
      ],
      "source": [
        "# Vectorize text\n",
        "start_time = time.time()\n",
        "X, vectorizer = vectorize_text(df)\n",
        "print(f\"Time taken to vectorize text: {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "# Perform KMeans clustering\n",
        "start_time = time.time()\n",
        "num_clusters = 5\n",
        "cluster_labels, kmeans = perform_kmeans(X, num_clusters)\n",
        "print(f\"Time taken for KMeans clustering: {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "# Dimensionality reduction for visualization\n",
        "start_time = time.time()\n",
        "svd = TruncatedSVD(n_components=2, random_state=42)\n",
        "X_svd = svd.fit_transform(X)\n",
        "print(f\"Time taken for dimensionality reduction: {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "# Add cluster labels to DataFrame\n",
        "df['Cluster_Labels'] = cluster_labels\n",
        "\n",
        "# Analyze important terms in each cluster\n",
        "start_time = time.time()\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
        "print_common_terms(terms, order_centroids,terms_per_cluster=15)\n",
        "print(f\"Time taken to analyze important terms: {time.time() - start_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "AaVlCC6eVKU7"
      },
      "outputs": [],
      "source": [
        "# DECIDED NOT TO PROCEED WITH DBSCAN CLUSTERING DUE TO ALGORITHM EFFICIENCY CONCERNS\n",
        "\n",
        "# Perform DBScan clustering as an alternative\n",
        "# start_time = time.time()\n",
        "# eps = 0.5\n",
        "# min_samples = 5\n",
        "# cluster_labels, dbscan = perform_dbscan(X, eps=eps, min_samples=min_samples)\n",
        "# print(f\"Time taken for DBSCAN clustering: {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "# # Dimensionality reduction for visualization\n",
        "# start_time = time.time()\n",
        "# svd = TruncatedSVD(n_components=2, random_state=42)\n",
        "# X_svd = svd.fit_transform(X)\n",
        "# print(f\"Time taken for dimensionality reduction: {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "# # Add cluster labels to DataFrame\n",
        "# df['Cluster_Labels_DBScan'] = cluster_labels\n",
        "\n",
        "# # Analyze important terms in each cluster\n",
        "# start_time = time.time()\n",
        "# print_common_terms_dbscan(df, terms, cluster_labels, top_terms=15)\n",
        "# print(f\"Time taken to analyze important terms: {time.time() - start_time:.2f} seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "u2GxQbu_5_kh",
        "outputId": "4215b003-2375-407a-cc5a-07680ddb00df"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'suggest_accounts' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-5c47f02a4f1d>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m }\n\u001b[1;32m      9\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_topic_suggestions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Time taken to apply topic suggestions: {time.time() - start_time:.2f} seconds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-e1be55faf186>\u001b[0m in \u001b[0;36mapply_topic_suggestions\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mapply_topic_suggestions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Topic_Description'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Cluster_Labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescribe_topic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Suggested_Accounts'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Topic_Description'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuggest_accounts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Cleaned_Text'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Cluster_Labels'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Topic_Description'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Suggested_Accounts'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'suggest_accounts' is not defined"
          ]
        }
      ],
      "source": [
        "# Apply the manually selected topic descriptions and user account suggestions to the DataFrame\n",
        "cluster_labels_mapping = {\n",
        "    0: 'Financial Markets and Stocks',\n",
        "    1: 'Public Health and Pandemic Updates',\n",
        "    2: 'Political Affairs and News',\n",
        "    3: 'Global Football Events',\n",
        "    4: 'Technology Trends and Innovations',\n",
        "}\n",
        "start_time = time.time()\n",
        "df = apply_topic_suggestions(df)\n",
        "print(f\"Time taken to apply topic suggestions: {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7BA7HmmR5Xxc"
      },
      "outputs": [],
      "source": [
        "# Save the DataFrame to an Excel file\n",
        "output_file_path = \"/content/drive/My Drive/4404MLProject/Processed_Data.csv\"\n",
        "df.to_csv(output_file_path, index=False)\n",
        "print(f\"DataFrame saved to {output_file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BhtzWbj73rSP"
      },
      "outputs": [],
      "source": [
        "# VISUALIZATIONS\n",
        "\n",
        "start_time = time.time()\n",
        "# Visualize clusters\n",
        "visualize_clusters(X_svd, cluster_labels)\n",
        "\n",
        "# Generate word clouds for each cluster\n",
        "generate_wordclouds(df, vectorizer)\n",
        "\n",
        "print(f\"Time taken for visualizations: {time.time() - start_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fYWRbRdsGbkn"
      },
      "outputs": [],
      "source": [
        "# Print topics and number of tweets for each topic\n",
        "topic_sizes = df['Topic_Description'].value_counts()\n",
        "for topic, size in topic_sizes.items():\n",
        "    print(f'Topic: {topic}, Number of Tweets: {size}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY6MHwsrxI5G"
      },
      "source": [
        "**DECIDED NOT TO PROCEED WITH SUBCLUSTERING CLUSTERING DUE TO INCONCLUSIVE AND MIXED SUBCLUSTERS**\n",
        "\n",
        "**Code is commented out**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "W54ntZeIk5H0"
      },
      "outputs": [],
      "source": [
        "major_topic_column_name = 'Topic_Description'\n",
        "num_clusters_subtopic = 3  # Adjust as needed\n",
        "\n",
        "# Train subtopic models\n",
        "subtopic_models = train_subtopic_models(df, major_topic_column=major_topic_column_name, num_clusters=num_clusters_subtopic)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eYsD5ZE6u3Tb"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\"subtopic_models:\", subtopic_models)\n",
        "# print(\"Labels shape:\", model_info['subset_df']['Subtopic_Labels'].shape)\n",
        "\n",
        "# # Evaluate subtopic models\n",
        "# evaluate_subtopic_models(subtopic_models)\n",
        "\n",
        "# Generate subtopic labels\n",
        "subtopic_labels = generate_subtopic_labels(subtopic_models)\n",
        "\n",
        "# Analyze subtopics\n",
        "analyze_subtopics(subtopic_models, subtopic_labels)\n",
        "\n",
        "# Example Usage\n",
        "# Assuming df is the DataFrame containing the tweets and Major_Topic and Subtopic_Labels columns\n",
        "# Also, assuming X_svd is the 2D representation of the vectors obtained using Truncated SVD\n",
        "\n",
        "# # Visualize subtopic clusters\n",
        "# for major_topic in subtopic_models.keys():\n",
        "#     visualize_subtopic_clusters(X_svd, subtopic_models[major_topic]['subset_df']['Subtopic_Labels'], major_topic)\n",
        "\n",
        "# Generate subtopic word clouds\n",
        "for major_topic in subtopic_models.keys():\n",
        "    generate_subtopic_wordclouds(df, vectorizer, subtopic_models, major_topic)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}